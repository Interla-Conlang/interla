<img width="100" height="100" alt="Interla Conlang Logo" src="https://github.com/user-attachments/assets/aa7a6017-25b6-4c08-aa5b-d6940cdee65f" />

# Interla Conlang Generation

## Step 1 : choosing the optimal subset of characters, and the pronunciation of each character
- Assuming that we use a subset of the roman alphabet, as decided [here]()
- Assuming we want only one pronunciation for each character, as decided [here]()

**Output**: This step produces a list of characters and their pronunciation, inside the file `alphabet.csv`.

## Step 2 : generating all possible interla spelled tokens
- The tokens are generated by combining the characters from the previous step, following a set of rules.

## Step 3 : training the interla model
- The model is trained by translating a language into another one, going through the interla language.
- We choose the vocabulary dimension.
**Output**: a translation model, and a list of anonymous tokens with their coocurrences with tokens from the source and target languages, inside the file `tokens.csv`.

## Step 4 : choosing the spelling of the anonymous tokens
- We optimize the spelling of the anonymous tokens, to minimize a cost function, based on the coocurrences with spelled words from the source and target languages.

**Output**: the list of anonymous tokens with their spelling, inside the file `vocabulary.csv`.

## Technical details

### Install
```bash
pip install -r requirements.txt
```
- [Install lex_lookup](https://pypi.org/project/epitran/#:~:text=Installation%20of%20Flite%20(for%20English%20G2P))

### Profile
```bash
kernprof -v -l script_to_profile.py
```

### Choosing edit distance
**Criterias**
- Very fast to compute (O(NÂ²) operations)
- Between 0 and 1, to be able to use it as a cost function

Libs:
- https://pypi.org/project/RapidFuzz/: SEEMS VERY FAST
- from difflib import SequenceMatcher, built-in
- ...

### Choosing minimum weight bipartite full matching algorithm
- [networkx minimum_weight_full_matching](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.bipartite.matching.minimum_weight_full_matching.html): graph takes too much space in memory
- [scipy min_weight_full_bipartite_matching](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.min_weight_full_bipartite_matching.html): CSR/COO graph matrix takes too much space in memory

## Tests
### 1) Choosing the optimal set of characters
No special tests here.

### 2) Generating all possible interla spelled tokens
No special tests here.

### 3) Training the interla model
We test on a small scale model (not a large LLM for example), to ensure that the concept works before scaling up.

### 4) Choosing the spelling of the anonymous tokens
Before we have the trained model from step 3, we can try to optimize the spelling of the anonymous tokens, by defining anonymous tokens from an existing languages, and coocurrences from real corpus.

I used word translations from [kakaobrain/word2word](https://github.com/kakaobrain/word2word).
I chose arbitrarily that Interla would replace "Estonian", i.e. we replace estonian tokens by interla learned tokens.

